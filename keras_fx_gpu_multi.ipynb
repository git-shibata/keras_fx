{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import urllib.request\n",
    "\n",
    "tf.set_random_seed(20170605)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "\n",
    "#スタート時間を保持\n",
    "starttime = time.time()\n",
    "\n",
    "# 学習の設定\n",
    "#l_of_s         = 20\n",
    "#n_next         = 5\n",
    "#l_of_s         = 20\n",
    "#n_next         = 5\n",
    "\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 300\n",
    "\n",
    "# 学習用データを抽出する関数\n",
    "def _load_data(data, n_prev=10, n_next=1, flag=True):\n",
    "    docX, docY = [], []\n",
    "    for i in range(n_prev, len(data)-n_next):\n",
    "        #起点の箇所からn_prevだけ戻った日数分をデータとする\n",
    "        docX.append(data.iloc[i-n_prev:i].as_matrix())\n",
    "        #起点の翌日からn_next日数分進んだデータのmax or minをyデータとする\n",
    "        if flag == True:\n",
    "            docY.append(max(data.iloc[i+1:i+1+n_next].as_matrix()))\n",
    "        else:\n",
    "            docY.append(min(data.iloc[i+1:i+1+n_next].as_matrix()))\n",
    "\n",
    "    alsX = numpy.array(docX)\n",
    "    alsY = numpy.array(docY)\n",
    "    return alsX, alsY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#data = None\n",
    "#data = pandas.read_csv(file_name)\n",
    "#data.columns = ['date', 'open', 'high', 'low', 'close']\n",
    "#data['date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "#close_open_diff = numpy.array( [] )\n",
    "#for i in range(len(data['date'])-1):\n",
    "#    close_open_diff = numpy.append(close_open_diff, data['close'][i] - data['open'][i+1])\n",
    "#\n",
    "#print('ave:' + str(numpy.mean(close_open_diff)))\n",
    "#print('std:' + str(numpy.std(close_open_diff)))\n",
    "#print('max:' + str(numpy.max(close_open_diff)))\n",
    "#print('min:' + str(numpy.min(close_open_diff)))\n",
    "\n",
    "#ave = numpy.mean(close_open_diff)\n",
    "#std = numpy.std(close_open_diff)\n",
    "\n",
    "#ave_count2 = 0\n",
    "#ave_count1 = 0\n",
    "#ave_count3 = 0\n",
    "\n",
    "#for i in range(len(data['date'])-1):\n",
    "#    if close_open_diff[i] > ave - std and \\\n",
    "#    close_open_diff[i] < ave + std:\n",
    "#        ave_count1 += 1\n",
    "#    if close_open_diff[i] > ave - std * 2 and \\\n",
    "#    close_open_diff[i] < ave + std * 2:\n",
    "#        ave_count2 += 1\n",
    "#    if close_open_diff[i] > ave - std * 3and \\\n",
    "#    close_open_diff[i] < ave + std * 3:\n",
    "#        ave_count3 += 1\n",
    "\n",
    "#print('total:' + str(len(data['date'])-1))\n",
    "#print('ave_count1: ' + str(ave_count1) + ' total:' + str(ave_count1/(len(data['date'])-1)))\n",
    "#print('ave_count2: ' + str(ave_count2) + ' total:' + str(ave_count2/(len(data['date'])-1)))\n",
    "#print('ave_count3: ' + str(ave_count3) + ' total:' + str(ave_count3/(len(data['date'])-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# l_of_s\n",
    "# n_next\n",
    "# check_treshhold\n",
    "# file_name\n",
    "# save_file_name\n",
    "# activation\n",
    "# loss_func\n",
    "# optimizer_func\n",
    "# validation_split_number\n",
    "def fit(l_of_s, n_next,check_treshhold,file_name,save_file_name,activation,\\\n",
    "        loss_func,optimizer_func,validation_split_number,\\\n",
    "        train_start_count, train_end_count,test_start_count, test_end_count,\\\n",
    "        data,average_value,diff_value,up_down,check_percent):\n",
    "\n",
    "\n",
    "    #グラフ化\n",
    "    #plt.plot(data['date'], data['high'])\n",
    "    #plt.plot(data['date'], data['low'])\n",
    "    #plt.show()\n",
    "\n",
    "    #sys.exit()\n",
    "    \n",
    "    #\n",
    "    #stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "    # データ準備\n",
    "    #ここではhigh/lowのtrain/testの４つを整備\n",
    "    #配列的にl_of_sとn_next分の前後にとる\n",
    "    X_high_train, y_high_train = _load_data(data[['high']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                            l_of_s, n_next, True)\n",
    "    X_high_test , y_high_test  = _load_data(data[['high']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                            l_of_s, n_next, True)\n",
    "    X_low_train , y_low_train  = _load_data(data[['low']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                            l_of_s, n_next, False)\n",
    "    X_low_test  , y_low_test   = _load_data(data[['low']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                            l_of_s, n_next, False)\n",
    "\n",
    "\n",
    "    # ニューラルネットの定義\n",
    "    #LSTMの定義/high/lowの両方を別々に保持したけど\n",
    "    #modeは共用してfitだけ違えればいい気がしないでもない\n",
    "    high_model = Sequential()\n",
    "    high_model.add(LSTM(hidden_neurons, \\\n",
    "              batch_input_shape=(None, l_of_s, in_out_neurons), \\\n",
    "              return_sequences=False))\n",
    "    high_model.add(Dense(in_out_neurons))\n",
    "    #high_model.add(BatchNormalization())\n",
    "    #high_model.add(Activation(\"linear\"))\n",
    "    high_model.add(Activation(activation))\n",
    "    high_model.compile(loss=loss_func, optimizer=optimizer_func)\n",
    "    #high_model.add(Activation(\"linear\"))\n",
    "    #high_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    low_model = Sequential()\n",
    "    low_model.add(LSTM(hidden_neurons, \\\n",
    "              batch_input_shape=(None, l_of_s, in_out_neurons), \\\n",
    "              return_sequences=False))\n",
    "    low_model.add(Dense(in_out_neurons))\n",
    "    #low_model.add(BatchNormalization())\n",
    "    #low_model.add(Activation(\"linear\"))\n",
    "    low_model.add(Activation(activation))\n",
    "    low_model.compile(loss=loss_func, optimizer=optimizer_func)\n",
    "    #low_model.add(Activation(\"linear\"))\n",
    "    #low_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    print(\"High Fit\")\n",
    "    # 学習\n",
    "    high_history = high_model.fit(X_high_train, y_high_train, batch_size=100, epochs=300, \\\n",
    "                   validation_split=validation_split_number, callbacks=[early_stopping])\n",
    "\n",
    "    # テスト結果表示\n",
    "    high_predicted = high_model.predict(X_high_test)\n",
    "\n",
    "    print(\"Low Fit\")\n",
    "    # 学習\n",
    "    low_history = low_model.fit(X_low_train, y_low_train, batch_size=100, epochs=300, \\\n",
    "                  validation_split=validation_split_number, callbacks=[early_stopping])\n",
    "\n",
    "    # テスト結果表示\n",
    "    low_predicted = low_model.predict(X_low_test)\n",
    "\n",
    "    # high\n",
    "    val_loss = high_history.history['val_loss']\n",
    "    plt.rc('font',family='serif')\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(len(high_history.history['val_loss'])), val_loss, label='val_loss', color='black')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.savefig('png/' + save_file_name + '_high_' + \\\n",
    "                str(l_of_s) + '_' + str(n_next) + \\\n",
    "                '_' + str(check_treshhold) + '_' + file_name + \\\n",
    "                '_' + activation + '_' + loss_func + \\\n",
    "                '_' + optimizer_func + '_' + str(validation_split_number) + \\\n",
    "                '.png')\n",
    "    plt.show()\n",
    "\n",
    "    # low\n",
    "    val_loss = low_history.history['val_loss']\n",
    "    plt.rc('font',family='serif')\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(len(low_history.history['val_loss'])), val_loss, label='val_loss', color='black')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.savefig('png/' + save_file_name + '_low__' + \\\n",
    "                str(l_of_s) + '_' + str(n_next) + \\\n",
    "                '_' + str(check_treshhold) + '_' + file_name + \\\n",
    "                '_' + activation + '_' + loss_func + \\\n",
    "                '_' + optimizer_func + '_' + str(validation_split_number) + \\\n",
    "                '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #high/lowの予想最大/最小値のグラフ（小さくてわからない）\n",
    "    result = pandas.DataFrame(high_predicted)\n",
    "    result.columns = ['high_predict']\n",
    "    result['low_predict'] = low_predicted\n",
    "    result.plot()\n",
    "    plt.savefig('png/' + save_file_name + '_result_' + \\\n",
    "                str(l_of_s) + '_' + str(n_next) + \\\n",
    "                '_' + str(check_treshhold) + '_' + file_name + \\\n",
    "                '_' + activation + '_' + loss_func + \\\n",
    "                '_' + optimizer_func + '_' + str(validation_split_number) + \\\n",
    "                '.png')\n",
    "    plt.show()\n",
    "\n",
    "    #\n",
    "    pre_check = []\n",
    "    temp_check = []\n",
    "    temp_close_open = []\n",
    "    temp_close_open_up_win = []\n",
    "    temp_close_open_up_lost = []\n",
    "    temp_close_open_down_win = []\n",
    "    temp_close_open_down_lost = []\n",
    "\n",
    "    #使わないデータも保持しているが\n",
    "    #予想したHighの最大値とLowの最小値を起点の日の翌日のOpenの\n",
    "    #と比較し、最初sに決定した差よりも大きい場合は上がり・下がりと\n",
    "    #判断する\n",
    "    #ただし、同時超えた場合はどちらが先に上がるか不明なためカウントしていない\n",
    "    #もしかすると\n",
    "    for i in range(len(low_predicted)):\n",
    "        high_temp  = high_predicted[i] * diff_value + average_value\n",
    "        low_temp   = low_predicted[i] * diff_value + average_value\n",
    "        open_temp  = data.loc[i+test_start_count+1, 'open'] * diff_value + average_value\n",
    "        close_temp = data.loc[i+test_start_count, 'close'] * diff_value + average_value\n",
    "\n",
    "        if high_temp - close_temp >= check_percent and \\\n",
    "            close_temp - low_temp < check_percent:\n",
    "            pre_check.append(1)\n",
    "            temp_check.append(high_temp - open_temp)\n",
    "            temp_close_open.append(abs(close_temp - open_temp))\n",
    "\n",
    "        elif close_temp- low_temp >= check_percent and \\\n",
    "            high_temp - close_temp < check_percent:\n",
    "            pre_check.append(-1)\n",
    "            temp_check.append(low_temp - open_temp)\n",
    "            temp_close_open.append(abs(close_temp - open_temp))\n",
    "        #elif high_temp - open_temp >= check_treshhold and \\\n",
    "        #    open_temp- low_temp >= check_treshhold:\n",
    "\n",
    "        #    if high_temp - open_temp > open_temp- low_temp:\n",
    "        #        pre_check.append(1)\n",
    "        #        temp_check.append(high_temp - open_temp)\n",
    "        #    elif high_temp - open_temp < open_temp- low_temp:\n",
    "        #        pre_check.append(-1)\n",
    "        #        temp_check.append(low_temp - open_temp)\n",
    "        #    else:\n",
    "        #        pre_check.append(0)\n",
    "        #        temp_check.append(0)\n",
    "\n",
    "        #if high_temp - open_temp >= check_treshhold and \\\n",
    "        #    open_temp - low_temp < check_treshhold and \\\n",
    "        #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "        #    close_temp - open_temp >= down_c_o_diff:\n",
    "        #    pre_check.append(1)\n",
    "        #    temp_check.append(high_temp - open_temp)\n",
    "\n",
    "        #elif open_temp- low_temp >= check_treshhold and \\\n",
    "        #    high_temp - open_temp < check_treshhold and \\\n",
    "        #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "        #    close_temp - open_temp >= down_c_o_diff:\n",
    "        #    pre_check.append(-1)\n",
    "        #    temp_check.append(low_temp - open_temp)\n",
    "\n",
    "        else:\n",
    "            pre_check.append(0)\n",
    "            temp_check.append(0)\n",
    "\n",
    "    up_ok_count = 0\n",
    "    up_ng_count = 0\n",
    "    up_ev_count = 0\n",
    "    down_ok_count = 0\n",
    "    down_ng_count = 0\n",
    "    down_ev_count = 0\n",
    "    high_win = numpy.array([])\n",
    "    high_lost = numpy.array([])\n",
    "    low_win  = numpy.array([])\n",
    "    low_lost  = numpy.array([])\n",
    "\n",
    "    for i in range(len(pre_check)):\n",
    "        if pre_check[i] == 1:\n",
    "            if up_down[i] == pre_check[i]:\n",
    "                up_ok_count += 1\n",
    "                high_win = numpy.append(high_win, numpy.array(temp_check[i]))\n",
    "            elif up_down[i] != pre_check[i] and up_down[i] == -1:\n",
    "                up_ng_count += 1\n",
    "                high_lost = numpy.append(high_lost, numpy.array(temp_check[i]))\n",
    "            else:\n",
    "                up_ev_count += 1\n",
    "\n",
    "        elif pre_check[i] == -1:\n",
    "            if up_down[i] == pre_check[i]:\n",
    "                down_ok_count += 1\n",
    "                low_win = numpy.append(low_win, numpy.array(temp_check[i]))\n",
    "            elif up_down[i] != pre_check[i] and up_down[i] == 1:\n",
    "                down_ng_count += 1\n",
    "                low_lost = numpy.append(low_lost, numpy.array(temp_check[i]))\n",
    "            else:\n",
    "                down_ev_count += 1\n",
    "\n",
    "\n",
    "    print('==========')\n",
    "    print('l_of_s: ' + str(l_of_s) + ' n_next: ' + str(n_next) + \\\n",
    "            ' check_treshhold:' + str(check_treshhold) + ' file_name:' + file_name + \\\n",
    "            ' activation:' + activation + ' loss_func:' + loss_func + \\\n",
    "            ' optimizer_func:' + optimizer_func + ' validation_split_number:' + str(validation_split_number))\n",
    "    print('UP:')\n",
    "    print(' WIN  :' + str(up_ok_count))\n",
    "    print(' LOST :' + str(up_ng_count))\n",
    "    print(' DRAW :' + str(up_ev_count))\n",
    "    print('DOWN:')\n",
    "    print(' WIN  :' + str(down_ok_count))\n",
    "    print(' LOST :' + str(down_ng_count))\n",
    "    print(' DRAW :' + str(down_ev_count))\n",
    "    \n",
    "    f = open('result/result_' + save_file_name + '.txt', 'a')\n",
    "    f.write('l_of_s: ' + str(l_of_s) + ' n_next: ' + str(n_next) + \\\n",
    "            ' check_treshhold:' + str(check_treshhold) + ' file_name:' + file_name + \\\n",
    "            ' activation:' + activation + ' loss_func:' + loss_func + \\\n",
    "            ' optimizer_func:' + optimizer_func + ' validation_split_number:' + str(validation_split_number) + \\\n",
    "            '\\n')\n",
    "    f.write('UP: ' + str(up_ok_count) + ' - ' + str(up_ng_count) + ' - ' + str(up_ev_count) + '\\n')\n",
    "    f.write('DN: ' + str(down_ok_count) + ' - ' + str(down_ng_count) + ' - ' + str(down_ev_count) + '\\n')\n",
    "    f.close()\n",
    "    #print('---------')\n",
    "    #print('UP ')\n",
    "    #print('  WIN  :' + str(high_win.mean()) + ' ' + str(high_win.std()))\n",
    "    #print('  LOST  :' + str(high_lost.mean()) + ' ' + str(high_lost.std()))\n",
    "    #print('DOWN ')\n",
    "    #print('  WIN  :' + str(low_win.mean()) + ' ' + str(low_win.std()))\n",
    "    #print('  LOST  :' + str(low_lost.mean()) + ' ' + str(low_lost.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data(l_of_s, n_next,check_treshhold,file_name,train_start_count, train_end_count,test_start_count, test_end_count,data):\n",
    "    # FXデータの読み込み\n",
    "    #data = None\n",
    "    #print(file_name)\n",
    "    #data = pandas.read_csv(file_name)\n",
    "    #data.columns = ['date', 'open', 'high', 'low', 'close']\n",
    "    #data['date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "    up_down           = []\n",
    "    up_count          = 0\n",
    "    down_count        = 0\n",
    "    even_count        = 0\n",
    "    #up/downの割合\n",
    "    loop_flag         = True\n",
    "\n",
    "    #\n",
    "    check_add_percent = 0.0020\n",
    "    check_percent     = 0.02\n",
    "    if min(data.loc[:, 'low']) > 150:\n",
    "        check_add_percent = 0.20\n",
    "        check_percent     = 2.0\n",
    "    elif min(data.loc[:, 'low']) > 30:\n",
    "        check_add_percent = 0.020\n",
    "        check_percent     = 0.2\n",
    "        #check_percent     = 0.1\n",
    "\n",
    "    #close_open_diff = numpy.array([])\n",
    "    #for i in range(train_start_count, test_end_count):\n",
    "    #    close_open_diff = numpy.append(close_open_diff, numpy.array(data.loc[i-1, 'close'] - data.loc[i, 'open']))\n",
    "    #print('Close-Open Diff:' + str(close_open_diff.mean()) + ' ' + str(close_open_diff.std()))\n",
    "    #up_c_o_diff   = close_open_diff.mean() + close_open_diff.std() * 2\n",
    "    #down_c_o_diff = close_open_diff.mean() - close_open_diff.std() * 2\n",
    "    #c_o_d_remove_count = 0\n",
    "    #for i in range(train_start_count, test_end_count):\n",
    "    #    if data.loc[i-1, 'close'] - data.loc[i, 'open'] >= up_c_o_diff or \\\n",
    "    #        data.loc[i-1, 'close'] - data.loc[i, 'open'] <= down_c_o_diff:\n",
    "    #        c_o_d_remove_count += 1\n",
    "    #print(\"c_o_d_remove_count:\" + str(c_o_d_remove_count))\n",
    "\n",
    "    high_flag = True\n",
    "    low_flag  = False\n",
    "    while loop_flag:\n",
    "        up_count = 0\n",
    "        down_count = 0\n",
    "        even_count = 0\n",
    "        check_percent += check_add_percent\n",
    "        for i in range(train_start_count, test_end_count):\n",
    "            #起点の日の翌日のopenの値\n",
    "            open_value = data.loc[i+1, 'open']\n",
    "            #起点の日から翌日からn_next日数分のhighの最大値\n",
    "            max_value = max(data.loc[i+1:i+1+n_next, 'high'])\n",
    "            #起点の日から翌日からn_next日数分のlowの最小値\n",
    "            min_value = min(data.loc[i+1:i+1+n_next, 'low'])\n",
    "            #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "            #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "            #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "            #回数を調べる\n",
    "\n",
    "            if max_value - open_value >= check_percent and \\\n",
    "                open_value - min_value < check_percent:\n",
    "                up_count += 1\n",
    "            elif open_value - min_value >= check_percent and \\\n",
    "                max_value - open_value < check_percent:\n",
    "                down_count += 1  \n",
    "            else:\n",
    "                even_count += 1\n",
    "\n",
    "        #(上がった日+下がった日)/全体の日数でcheck_treshholdを超えたか調べる\n",
    "        #超えていればその値をベースにする\n",
    "        print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                                 (up_count + down_count +even_count) * 100)) + '%')    \n",
    "        #if high_flag == True:\n",
    "        #    if (up_count + down_count) / (up_count + down_count +even_count) > check_treshhold:\n",
    "        #        high_flag = False\n",
    "        #        low_flag = True\n",
    "        #elif low_flag == True:\n",
    "        #    if (up_count + down_count) / (up_count + down_count +even_count) < check_treshhold:\n",
    "        #        break\n",
    "        if (up_count + down_count) / (up_count + down_count +even_count) > check_treshhold:\n",
    "            break\n",
    "\n",
    "    #上がった・下がったの判定するための数字（もっと言うと円がどれだけ動いたか）\n",
    "    print('p            : ' + str(check_percent))\n",
    "    print(\"UP   COUNT   : \" + str(up_count))\n",
    "    print(\"DOWN COUNT   : \" + str(down_count))\n",
    "    print(\"EVNE COUNT   : \" + str(even_count))\n",
    "    print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                             (up_count + down_count +even_count) * 100)) + '%')    \n",
    "\n",
    "    #sys.exit()\n",
    "    #上がった・下がったの判定用\n",
    "    #ここではtestの範囲のみ取得\n",
    "    for i in range(test_start_count, test_end_count):\n",
    "        open_value = data.loc[i+1, 'open']\n",
    "        close_value = data.loc[i, 'open']\n",
    "        #起点の日から翌日からn_next日数分のhighの最大値\n",
    "        max_value = max(data.loc[i+1:i+1+n_next, 'high'])\n",
    "        #起点の日から翌日からn_next日数分のlowの最小値\n",
    "        min_value = min(data.loc[i+1:i+1+n_next, 'low'])\n",
    "        #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "        #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "        #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "        #回数を調べる\n",
    "        if max_value - open_value >= check_percent and \\\n",
    "            open_value - min_value < check_percent:\n",
    "            up_down.append(1)\n",
    "        elif open_value - min_value >= check_percent and \\\n",
    "            max_value - open_value < check_percent:\n",
    "            up_down.append(-1)\n",
    "        else:\n",
    "            up_down.append(0)\n",
    "\n",
    "    #正規化\n",
    "    max_value = max(data['high'])\n",
    "    min_value = min(data['low'])\n",
    "    average_value = (max_value+min_value)/2\n",
    "    diff_value = max_value - average_value\n",
    "    print ('max: ' + str(max_value))\n",
    "    print ('min: ' + str(min_value))\n",
    "    print ('ave: ' + str(average_value))\n",
    "    print ('dif: ' + str(diff_value))\n",
    "\n",
    "    temp_data = data.copy()\n",
    "    for i in range(len(temp_data['high'].index)):\n",
    "        temp_data.loc[i, 'high'] = (temp_data.loc[i, 'high'] - average_value) / diff_value\n",
    "        temp_data.loc[i, 'low']  = (temp_data.loc[i, 'low'] - average_value) / diff_value\n",
    "        temp_data.loc[i, 'open'] = (temp_data.loc[i, 'open'] - average_value) / diff_value\n",
    "        temp_data.loc[i, 'close'] = (temp_data.loc[i, 'close'] - average_value) / diff_value\n",
    "\n",
    "    temp_data = temp_data.sort_values(by='date')\n",
    "    temp_data = temp_data.reset_index(drop=True)\n",
    "    temp_data = temp_data.loc[:, ['date', 'open','high', 'low', 'close']]\n",
    "\n",
    "    return (temp_data,average_value,diff_value, up_down,check_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_date(start_day, end_day, currency_pair):\n",
    "    #スタート日付\n",
    "    #終了日を今日に指定\n",
    "    url       = \"https://stooq.com/q/d/l/?s=\" + currency_pair + \\\n",
    "                \"&d1=\" + start_day + \"&d2=\" + end_day + \"&i=d\"\n",
    "    file_name = currency_pair + '_d.csv'\n",
    "    #取得して、ファイルに保存(よくよく考えると保存しなくてもいいな)\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    # FXデータの読み込み\n",
    "    data = None\n",
    "    #print(file_name)\n",
    "    data = pandas.read_csv(file_name)\n",
    "    data.columns = ['date', 'open', 'high', 'low', 'close']\n",
    "    data['date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "    #train開始終了、test開始終了日を設定\n",
    "    #データは2001/01/01からとっていますが、さかのぼって調査する関係上\n",
    "    #20001/01/01からにしています。また買い目対象の調査としてtest_end_dayよりも\n",
    "    #後の日も参照するためtest_end_dayの取得するデータにはしないでください。\n",
    "    #trainの終了日とtestの開始日が連続していますが、連続させなくても動作するか\n",
    "    #検証していません。注意してください\n",
    "    train_start_day   = dt.strptime('2002-01-01', '%Y-%m-%d')\n",
    "    train_end_day     = dt.strptime('2013-12-31', '%Y-%m-%d')\n",
    "    test_start_day    = dt.strptime('2014-01-01', '%Y-%m-%d')\n",
    "    test_end_day      = dt.strptime('2016-12-31', '%Y-%m-%d')\n",
    "    train_start_count = -1\n",
    "    train_end_count   = -1\n",
    "    test_start_count  = -1\n",
    "    test_end_count    = -1\n",
    "\n",
    "    #train/testの開始終了日の配列の場所を調査\n",
    "    for i in range(len(data['date'])):\n",
    "        if train_start_count == -1 and data['date'][i] >= train_start_day:\n",
    "            train_start_count = i\n",
    "        if train_end_count == -1 and data['date'][i] >= train_end_day:\n",
    "            train_end_count = i\n",
    "        if test_start_count == -1 and data['date'][i] >= test_start_day:\n",
    "            test_start_count = i\n",
    "        if test_end_count == -1 and data['date'][i] >= test_end_day:\n",
    "            test_end_count = i\n",
    "            break\n",
    "\n",
    "    #前にl_of_s日分、後ろにn_next分日数が必要なので\n",
    "    #チェック。足りない場合は中止。これを考慮に入れて\n",
    "    #train/testの開始終了日を設定してください\n",
    "    #if train_start_count - l_of_s < 0 or \\\n",
    "    #    test_end_count + n_next > len(data['date']):\n",
    "\n",
    "    #    print(\"data range over\")\n",
    "    #    sys.exit()\n",
    "\n",
    "    print('Train Start: ' + str(train_start_count))\n",
    "    print('Train End  : ' + str(train_end_count))\n",
    "    print('Test  Start: ' + str(test_start_count))\n",
    "    print('Test  End  : ' + str(test_end_count))\n",
    "\n",
    "    return (train_start_count, train_end_count,test_start_count, test_end_count,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "l_of_s_list                  = [20,25]\n",
    "n_next_list                  = [5,7]\n",
    "check_treshhold_list         = [0.50,0.60]\n",
    "#activation_list              = ['sigmoid','tanh','linear']\n",
    "activation_list              = ['tanh']\n",
    "#loss_func_list               = ['mean_squared_error','mean_absolute_error','mean_squared_logarithmic_error']\n",
    "loss_func_list               = ['mean_squared_error','mean_absolute_error']\n",
    "#optimizer_func_list          = ['sgd','adadelta','adam','adamax']\n",
    "optimizer_func_list          = ['adadelta','adam','adamax']\n",
    "#validation_split_number_list = [0.1,0.05]\n",
    "validation_split_number_list = [0.05]\n",
    "\n",
    "#l_of_s_list                  = [20]\n",
    "#n_next_list                  = [5]\n",
    "#check_treshhold_list         = [0.5]\n",
    "#activation_list              = ['linear']\n",
    "#loss_func_list               = ['mean_squared_error']\n",
    "#optimizer_func_list          = ['adam']\n",
    "#validation_split_number_list = [0.10]\n",
    "\n",
    "currency_pair_list   = ['usdjpy']\n",
    "\n",
    "# 結果ファイルの格納\n",
    "if os.path.exists('result') == False:\n",
    "    os.mkdir('result')\n",
    "if os.path.exists('png') == False:\n",
    "    os.mkdir('png')\n",
    "\n",
    "save_file_name = 'result/result_' + dt.today().strftime(\"%Y%m%d%H%M%S\") + '.txt'\n",
    "save_file_name = dt.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# fxのデータ取得\n",
    "start_day     = \"20010101\"\n",
    "end_day       =  dt.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "for currency_pair in currency_pair_list:\n",
    "    (train_start_count, train_end_count,test_start_count, test_end_count,data) = \\\n",
    "        get_date(start_day, end_day, currency_pair)\n",
    "    file_name = currency_pair + '_d.csv'\n",
    "\n",
    "    for l_of_s in l_of_s_list:\n",
    "        for n_next in n_next_list:\n",
    "            for check_treshhold in check_treshhold_list:\n",
    "                #\n",
    "                (chane_data,average_value,diff_value, up_down,check_percent) = \\\n",
    "                    get_data(l_of_s, n_next,check_treshhold, file_name,train_start_count,\\\n",
    "                             train_end_count,test_start_count, test_end_count,data)\n",
    "                \n",
    "                #\n",
    "                for activation in activation_list:\n",
    "                    for loss_func in loss_func_list:\n",
    "                        for optimizer_func in optimizer_func_list:\n",
    "                            for validation_split_number in validation_split_number_list:\n",
    "                                #print('l_of_s: ' + str(l_of_s) + ' n_next:' + str(n_next) + ' check_treshhold:' + str(check_treshhold) \\\n",
    "                                #  + ' file_name:' + file_name)\n",
    "                                print('--------------------------')\n",
    "                                fit_starttime = time.time()\n",
    "                                fit(l_of_s, n_next,check_treshhold,file_name,save_file_name,activation,loss_func,optimizer_func,\\\n",
    "                                    validation_split_number,train_start_count, train_end_count,test_start_count, test_end_count,\\\n",
    "                                    chane_data,average_value,diff_value,up_down,check_percent)\n",
    "                                print(str(math.floor(time.time() - fit_starttime)) + \"s\")\n",
    "                                print('')\n",
    "\n",
    "\n",
    "print(str(math.floor(time.time() - starttime)) + \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
